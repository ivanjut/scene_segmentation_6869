{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import jaccard_score as IOU\n",
    "from torchvision import models, transforms, io\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "import utils\n",
    "import os\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATASET_PATH = 'ADE20K_2021_17_01/'\n",
    "index_file = 'index_ade20k.pkl'\n",
    "with open('{}/{}'.format(DATASET_PATH, index_file), 'rb') as f:\n",
    "    index_ade20k = pkl.load(f)\n",
    "\n",
    "objects_mat = index_ade20k['objectPresence']\n",
    "\n",
    "# Find 150 most common object IDs and non-common object IDs\n",
    "total_object_counts = np.sum(objects_mat, axis=1)\n",
    "object_count_ids = np.argsort(total_object_counts)[::-1]\n",
    "most_common_obj_ids = object_count_ids[:150]\n",
    "irrelevant_obj_ids = object_count_ids[150:]\n",
    "# Find image IDs where no irrelevant objects appear\n",
    "irrelevant_obj_counts = np.sum(objects_mat[irrelevant_obj_ids], axis=0)\n",
    "good_image_ids = np.argwhere(irrelevant_obj_counts == 0).flatten()\n",
    "# Only common objects included\n",
    "common_objects_mat = objects_mat[np.ix_(most_common_obj_ids, good_image_ids)]\n",
    "\n",
    "# Maps {obj_ids: 0-149}\n",
    "obj_id_map = {sorted(most_common_obj_ids)[idx]: idx + 1 for idx in range(150)}\n",
    "obj_id_map[-1] = 0\n",
    "\n",
    "# Pick out images to train/evaluate on\n",
    "train_image_ids = []\n",
    "test_image_ids = []\n",
    "for i in good_image_ids:\n",
    "    if 'training' in index_ade20k['folder'][i]:\n",
    "        train_image_ids.append(i)\n",
    "    elif 'validation' in index_ade20k['folder'][i]:\n",
    "        test_image_ids.append(i)\n",
    "    else:\n",
    "        raise Exception('Invalid folder name.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_ids, root_dir, index_mat, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_ids (list): list of image IDs from ADE20K\n",
    "            root_dir (string): Directory with all the images.\n",
    "            index_mat (array): object array from index_ade20k.pkl\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            target_transform (callable, optional): Optional transform to be applied\n",
    "                on a sample segmentation label.\n",
    "        \"\"\"\n",
    "        self.image_ids = image_ids\n",
    "        self.root_dir = root_dir\n",
    "        self.index_ade20k = index_mat\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_name = os.path.join(self.root_dir, self.index_ade20k['folder'][image_id], \n",
    "                                self.index_ade20k['filename'][image_id])\n",
    "        img_info = utils.loadAde20K(img_name)\n",
    "        \n",
    "        image = io.read_image(img_info['img_name']).float()\n",
    "        class_mask = Image.fromarray(img_info['class_mask'], mode='I')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(class_mask)\n",
    "            \n",
    "        sample = (image, label)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "input_size = 224\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize(input_size),\n",
    "                transforms.CenterCrop(input_size),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "                transforms.Resize(input_size, interpolation=0),\n",
    "                transforms.CenterCrop(input_size),\n",
    "                transforms.ToTensor()\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(test_image_ids)\n",
    "batch_size = 32\n",
    "training_data = SegmentationDataset(train_image_ids[:num_samples], './', index_ade20k, transform=transform, target_transform=target_transform)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=False)\n",
    "testing_data = SegmentationDataset(test_image_ids[:num_samples], './', index_ade20k, transform=transform, target_transform=target_transform)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label_arr, obj_id_map):\n",
    "    \"\"\"\n",
    "    Encode labels for evaluating loss\n",
    "    label_arr (tensor): B x 1 x H x W\n",
    "    obj_id_map: dictionary mapping label class IDs to new (0-150) range IDs\n",
    "    \"\"\"\n",
    "    convert_label_ids = lambda i: obj_id_map[i-1]\n",
    "    vect_convert_label_ids = np.vectorize(convert_label_ids)\n",
    "    encoded_label = vect_convert_label_ids(label_arr.squeeze().cpu().numpy())\n",
    "    \n",
    "    return torch.tensor(encoded_label, dtype=torch.long)\n",
    "\n",
    "def get_parameter_size(model):\n",
    "    \"\"\"\n",
    "    Return model size in terms of parameters\n",
    "    Each parameter is a float32 - 4 bytes\n",
    "    \"\"\"\n",
    "    num_params = 0\n",
    "    for p in model.parameters():\n",
    "        num_params += torch.count_nonzero(p.flatten())\n",
    "        \n",
    "    total_bytes = num_params.item() / 4\n",
    "    kb = total_bytes / 1000\n",
    "    \n",
    "    return {\"# Params\": num_params.item(),\n",
    "            \"Size in KB\": kb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, test_dataloader):\n",
    "    # testing pass\n",
    "    test_start = time.time()\n",
    "    running_accuracy = 0\n",
    "    running_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images = images.to(device)\n",
    "            output = model(images)['out']\n",
    "            labels = encode_label(labels, obj_id_map).to(device)\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1, keepdim=True).squeeze()\n",
    "            num_correct = torch.sum((preds == labels).to(int)).item()\n",
    "            iou = IOU(labels.detach().cpu().numpy().reshape(-1), preds.detach().cpu().numpy().reshape(-1), average='weighted')\n",
    "            print('Testing accuracy: {}'.format(num_correct/(224*224*len(images))))\n",
    "            print('Testing IOU score: {}'.format(iou))\n",
    "            running_accuracy += num_correct/(224*224*len(images))\n",
    "            running_iou += iou\n",
    "\n",
    "    print(\"Testing time: {} seconds\".format(time.time() - test_start))\n",
    "\n",
    "    return {\"Testing pixel accuracy\": running_accuracy / len(test_dataloader),\n",
    "            \"Testing IOU accuracy\": running_iou / len(test_dataloader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.segmentation.fcn_resnet50(pretrained=False, num_classes=151).to(device=device)\n",
    "model.load_state_dict(torch.load('../epochs_20_weights.pkl', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.6961252640704719\n",
      "Testing IOU score: 0.5567664266454655\n",
      "Testing accuracy: 0.5719585807955995\n",
      "Testing IOU score: 0.4300693239409355\n",
      "Testing accuracy: 0.6724398865991709\n",
      "Testing IOU score: 0.5434137677671387\n",
      "Testing accuracy: 0.5736245914381377\n",
      "Testing IOU score: 0.4454706434037535\n",
      "Testing accuracy: 0.6390131736288265\n",
      "Testing IOU score: 0.49749971505331414\n",
      "Testing accuracy: 0.6801969567123725\n",
      "Testing IOU score: 0.569534708735073\n",
      "Testing accuracy: 0.6379326022401148\n",
      "Testing IOU score: 0.49175185504711705\n",
      "Testing accuracy: 0.6650147729990433\n",
      "Testing IOU score: 0.5234921624351464\n",
      "Testing accuracy: 0.6555281658561862\n",
      "Testing IOU score: 0.5117365614936726\n",
      "Testing accuracy: 0.6544855442176871\n",
      "Testing IOU score: 0.5226432431360427\n",
      "Testing time: 289.6454048156738 seconds\n"
     ]
    }
   ],
   "source": [
    "validation_metrics = validate(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Testing pixel accuracy': 0.644631953855761,\n",
       " 'Testing IOU accuracy': 0.5092378407657658}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
