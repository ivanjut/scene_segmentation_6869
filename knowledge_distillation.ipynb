{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import jaccard_score as IOU\n",
    "from torchvision import models, transforms, io\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "import utils\n",
    "import os\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATASET_PATH = 'ADE20K_2021_17_01/'\n",
    "index_file = 'index_ade20k.pkl'\n",
    "with open('{}/{}'.format(DATASET_PATH, index_file), 'rb') as f:\n",
    "    index_ade20k = pkl.load(f)\n",
    "\n",
    "objects_mat = index_ade20k['objectPresence']\n",
    "\n",
    "# Find 150 most common object IDs and non-common object IDs\n",
    "total_object_counts = np.sum(objects_mat, axis=1)\n",
    "object_count_ids = np.argsort(total_object_counts)[::-1]\n",
    "most_common_obj_ids = object_count_ids[:150]\n",
    "irrelevant_obj_ids = object_count_ids[150:]\n",
    "# Find image IDs where no irrelevant objects appear\n",
    "irrelevant_obj_counts = np.sum(objects_mat[irrelevant_obj_ids], axis=0)\n",
    "good_image_ids = np.argwhere(irrelevant_obj_counts == 0).flatten()\n",
    "# Only common objects included\n",
    "common_objects_mat = objects_mat[np.ix_(most_common_obj_ids, good_image_ids)]\n",
    "\n",
    "# Maps {obj_ids: 0-149}\n",
    "obj_id_map = {sorted(most_common_obj_ids)[idx]: idx + 1 for idx in range(150)}\n",
    "obj_id_map[-1] = 0\n",
    "\n",
    "# Pick out images to train/evaluate on\n",
    "train_image_ids = []\n",
    "test_image_ids = []\n",
    "for i in good_image_ids:\n",
    "    if 'training' in index_ade20k['folder'][i]:\n",
    "        train_image_ids.append(i)\n",
    "    elif 'validation' in index_ade20k['folder'][i]:\n",
    "        test_image_ids.append(i)\n",
    "    else:\n",
    "        raise Exception('Invalid folder name.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_ids, root_dir, index_mat, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_ids (list): list of image IDs from ADE20K\n",
    "            root_dir (string): Directory with all the images.\n",
    "            index_mat (array): object array from index_ade20k.pkl\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            target_transform (callable, optional): Optional transform to be applied\n",
    "                on a sample segmentation label.\n",
    "        \"\"\"\n",
    "        self.image_ids = image_ids\n",
    "        self.root_dir = root_dir\n",
    "        self.index_ade20k = index_mat\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_name = os.path.join(self.root_dir, self.index_ade20k['folder'][image_id], \n",
    "                                self.index_ade20k['filename'][image_id])\n",
    "        img_info = utils.loadAde20K(img_name)\n",
    "        \n",
    "        image = io.read_image(img_info['img_name']).float()\n",
    "        class_mask = Image.fromarray(img_info['class_mask'], mode='I')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(class_mask)\n",
    "            \n",
    "        sample = (image, label)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "input_size = 224\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize(input_size),\n",
    "                transforms.CenterCrop(input_size),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "                transforms.Resize(input_size, interpolation=0),\n",
    "                transforms.CenterCrop(input_size),\n",
    "                transforms.ToTensor()\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 4\n",
    "batch_size = 2\n",
    "training_data = SegmentationDataset(train_image_ids[:num_samples], './', index_ade20k, transform=transform, target_transform=target_transform)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=False)\n",
    "testing_data = SegmentationDataset(test_image_ids[:num_samples], './', index_ade20k, transform=transform, target_transform=target_transform)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label_arr, obj_id_map):\n",
    "    \"\"\"\n",
    "    Encode labels for evaluating loss\n",
    "    label_arr (tensor): B x 1 x H x W\n",
    "    obj_id_map: dictionary mapping label class IDs to new (0-150) range IDs\n",
    "    \"\"\"\n",
    "    convert_label_ids = lambda i: obj_id_map[i-1]\n",
    "    vect_convert_label_ids = np.vectorize(convert_label_ids)\n",
    "    encoded_label = vect_convert_label_ids(label_arr.squeeze().cpu().numpy())\n",
    "    \n",
    "    return torch.tensor(encoded_label, dtype=torch.long)\n",
    "\n",
    "def get_parameter_size(model):\n",
    "    \"\"\"\n",
    "    Return model size in terms of parameters\n",
    "    Each parameter is a float32 - 4 bytes\n",
    "    \"\"\"\n",
    "    num_params = 0\n",
    "    for p in model.parameters():\n",
    "        num_params += torch.count_nonzero(p.flatten())\n",
    "        \n",
    "    total_bytes = num_params.item() * 4\n",
    "    kb = total_bytes / 1000\n",
    "    \n",
    "    return {\"# Params\": num_params.item(),\n",
    "            \"Size in KB\": kb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, test_dataloader):\n",
    "    # testing pass\n",
    "    test_start = time.time()\n",
    "    running_accuracy = 0\n",
    "    running_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images = images.to(device)\n",
    "            output = model(images)['out']\n",
    "            labels = encode_label(labels, obj_id_map).to(device)\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1, keepdim=True).squeeze()\n",
    "            num_correct = torch.sum((preds == labels).to(int)).item()\n",
    "            iou = IOU(labels.detach().cpu().numpy().reshape(-1), preds.detach().cpu().numpy().reshape(-1), average='weighted')\n",
    "            print('Testing accuracy: {}'.format(num_correct/(224*224*len(images))))\n",
    "            print('Testing IOU score: {}'.format(iou))\n",
    "            running_accuracy += num_correct/(224*224*len(images))\n",
    "            running_iou += iou\n",
    "\n",
    "    print(\"Testing time: {} seconds\".format(time.time() - test_start))\n",
    "\n",
    "    return {\"Testing pixel accuracy\": running_accuracy / len(test_dataloader),\n",
    "            \"Testing IOU accuracy\": running_iou / len(test_dataloader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdPruning(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def compute_mask(self, tensor, default_mask):\n",
    "        return torch.abs(tensor) > self.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load teacher models\n",
    "teacher_model_1 = models.segmentation.fcn_resnet50(pretrained=False, num_classes=151).to(device)\n",
    "teacher_model_1.load_state_dict(torch.load('../scene_seg_models/fcn_resnet_50/epochs_20_weights.pkl', map_location=torch.device('cpu')))\n",
    "teacher_model_2 = models.segmentation.deeplabv3_resnet50(pretrained=False, num_classes=151).to(device)\n",
    "teacher_model_2.load_state_dict(torch.load('../scene_seg_models/deeplab_resnet_50/epochs_20_weights.pkl', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(teacher_model_1) # fcn resnet 50\n",
    "\n",
    "params_to_prune = [(module, \"weight\") for _, module in model.named_modules() if isinstance(module, torch.nn.Conv2d)]\n",
    "prune.global_unstructured(params_to_prune, pruning_method=ThresholdPruning, threshold=0.025)\n",
    "for _, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.remove(module, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, teacher_models, train_dataloader, test_dataloader, obj_id_map, epochs=5, lr=0.1, momentum=0.8):\n",
    "\n",
    "    train_start = time.time()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # training pass\n",
    "        running_loss = 0\n",
    "        for images, _ in train_dataloader:\n",
    "            images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            shape, full_tensor = None, None\n",
    "            for tm in teacher_models:\n",
    "                soft_label = tm(images)['out']\n",
    "                if shape is None:\n",
    "                    shape = soft_label.size()\n",
    "                    full_tensor = torch.zeros(shape)\n",
    "                full_tensor = full_tensor.add(soft_label)\n",
    "            soft_label_outputs = torch.div(full_tensor, len(teacher_models))\n",
    "            label_probs = torch.nn.functional.softmax(soft_label_outputs, dim=1)\n",
    "            labels = torch.argmax(label_probs, dim=1, keepdim=True).squeeze()\n",
    "                        \n",
    "            output = model(images)['out']\n",
    "#             labels = encode_label(labels, obj_id_map).to(device)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            print('Batch finished...')\n",
    "        print('Training loss: {}'.format(running_loss/len(train_dataloader)))\n",
    "        print(\"Training time: {} seconds\".format(time.time() - epoch_start))\n",
    "#         torch.save(model, result_path+'/epochs_{}_model.pkl'.format(i+1))\n",
    "\n",
    "        # testing pass\n",
    "        test_start = time.time()\n",
    "        running_accuracy = 0\n",
    "        running_iou = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_dataloader:\n",
    "                images = images.to(device)\n",
    "                output = model(images)['out']\n",
    "                labels = encode_label(labels, obj_id_map).to(device)\n",
    "                probs = torch.nn.functional.softmax(output, dim=1)\n",
    "                preds = torch.argmax(probs, dim=1, keepdim=True).squeeze()\n",
    "                num_correct = torch.sum((preds == labels).to(int)).item()\n",
    "                iou = IOU(labels.detach().cpu().numpy().reshape(-1), preds.detach().cpu().numpy().reshape(-1), average='weighted')\n",
    "                print('Testing accuracy: {}'.format(num_correct/(224*224*len(images))))\n",
    "                print('Testing IOU score: {}'.format(iou))\n",
    "                running_accuracy += num_correct/(224*224*len(images))\n",
    "                running_iou += iou\n",
    "        print(\"Testing time: {} seconds\".format(time.time() - test_start))\n",
    "        print('-----> Overall testing pixel accuracy: {}'.format(running_accuracy / len(test_dataloader)))\n",
    "        print('-----> Overall testing IOU accuracy: {}'.format(running_iou / len(test_dataloader)))\n",
    "        print(\"Epoch completed in {} seconds.\".format(time.time() - epoch_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch finished...\n",
      "Batch finished...\n",
      "Training loss: 6.192278861999512\n",
      "Training time: 13.007899045944214 seconds\n",
      "Testing accuracy: 0.0057597257653061226\n",
      "Testing IOU score: 0.005636417137165831\n",
      "Testing accuracy: 0.011041135204081632\n",
      "Testing IOU score: 0.010727063310563272\n",
      "Testing time: 2.285918951034546 seconds\n",
      "-----> Overall testing pixel accuracy: 0.008400430484693877\n",
      "-----> Overall testing IOU accuracy: 0.008181740223864552\n",
      "Epoch completed in 15.29406213760376 seconds.\n",
      "Batch finished...\n",
      "Batch finished...\n",
      "Training loss: 5.117037057876587\n",
      "Training time: 12.4536612033844 seconds\n",
      "Testing accuracy: 0.39877630739795916\n",
      "Testing IOU score: 0.16006419993060034\n",
      "Testing accuracy: 0.32359095982142855\n",
      "Testing IOU score: 0.10545073814342741\n",
      "Testing time: 2.4337868690490723 seconds\n",
      "-----> Overall testing pixel accuracy: 0.36118363360969385\n",
      "-----> Overall testing IOU accuracy: 0.13275746903701388\n",
      "Epoch completed in 14.887526035308838 seconds.\n",
      "Batch finished...\n",
      "Batch finished...\n",
      "Training loss: 5.532170057296753\n",
      "Training time: 12.668865203857422 seconds\n",
      "Testing accuracy: 0.3545719068877551\n",
      "Testing IOU score: 0.15225256876860893\n",
      "Testing accuracy: 0.32143853635204084\n",
      "Testing IOU score: 0.11712524515655454\n",
      "Testing time: 2.624006986618042 seconds\n",
      "-----> Overall testing pixel accuracy: 0.338005221619898\n",
      "-----> Overall testing IOU accuracy: 0.13468890696258173\n",
      "Epoch completed in 15.292968034744263 seconds.\n",
      "Batch finished...\n",
      "Batch finished...\n",
      "Training loss: 5.1740288734436035\n",
      "Training time: 16.764209985733032 seconds\n",
      "Testing accuracy: 0.20738998724489796\n",
      "Testing IOU score: 0.08422390266323164\n",
      "Testing accuracy: 0.052435427295918366\n",
      "Testing IOU score: 0.011368424283853429\n",
      "Testing time: 3.681807279586792 seconds\n",
      "-----> Overall testing pixel accuracy: 0.12991270727040816\n",
      "-----> Overall testing IOU accuracy: 0.047796163473542534\n",
      "Epoch completed in 20.44616675376892 seconds.\n",
      "Batch finished...\n",
      "Batch finished...\n",
      "Training loss: 3.655452847480774\n",
      "Training time: 20.731022834777832 seconds\n",
      "Testing accuracy: 0.3096301020408163\n",
      "Testing IOU score: 0.14570112685738607\n",
      "Testing accuracy: 0.0668148118622449\n",
      "Testing IOU score: 0.022861915398535167\n",
      "Testing time: 4.054157018661499 seconds\n",
      "-----> Overall testing pixel accuracy: 0.18822245695153061\n",
      "-----> Overall testing IOU accuracy: 0.08428152112796061\n",
      "Epoch completed in 24.785335779190063 seconds.\n"
     ]
    }
   ],
   "source": [
    "test_model = copy.deepcopy(model)\n",
    "train_model(test_model, [teacher_model_1, teacher_model_2], train_dataloader, test_dataloader, obj_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
