{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import jaccard_score as IOU\n",
    "from torchvision import models, transforms, io\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "import utils\n",
    "import os\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATASET_PATH = 'ADE20K_2021_17_01/'\n",
    "index_file = 'index_ade20k.pkl'\n",
    "with open('{}/{}'.format(DATASET_PATH, index_file), 'rb') as f:\n",
    "    index_ade20k = pkl.load(f)\n",
    "\n",
    "objects_mat = index_ade20k['objectPresence']\n",
    "\n",
    "# Find 150 most common object IDs and non-common object IDs\n",
    "total_object_counts = np.sum(objects_mat, axis=1)\n",
    "object_count_ids = np.argsort(total_object_counts)[::-1]\n",
    "most_common_obj_ids = object_count_ids[:150]\n",
    "irrelevant_obj_ids = object_count_ids[150:]\n",
    "# Find image IDs where no irrelevant objects appear\n",
    "irrelevant_obj_counts = np.sum(objects_mat[irrelevant_obj_ids], axis=0)\n",
    "good_image_ids = np.argwhere(irrelevant_obj_counts == 0).flatten()\n",
    "# Only common objects included\n",
    "common_objects_mat = objects_mat[np.ix_(most_common_obj_ids, good_image_ids)]\n",
    "\n",
    "# Maps {obj_ids: 0-149}\n",
    "obj_id_map = {sorted(most_common_obj_ids)[idx]: idx + 1 for idx in range(150)}\n",
    "obj_id_map[-1] = 0\n",
    "\n",
    "# Pick out images to train/evaluate on\n",
    "train_image_ids = []\n",
    "test_image_ids = []\n",
    "for i in good_image_ids:\n",
    "    if 'training' in index_ade20k['folder'][i]:\n",
    "        train_image_ids.append(i)\n",
    "    elif 'validation' in index_ade20k['folder'][i]:\n",
    "        test_image_ids.append(i)\n",
    "    else:\n",
    "        raise Exception('Invalid folder name.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_ids, root_dir, index_mat, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_ids (list): list of image IDs from ADE20K\n",
    "            root_dir (string): Directory with all the images.\n",
    "            index_mat (array): object array from index_ade20k.pkl\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            target_transform (callable, optional): Optional transform to be applied\n",
    "                on a sample segmentation label.\n",
    "        \"\"\"\n",
    "        self.image_ids = image_ids\n",
    "        self.root_dir = root_dir\n",
    "        self.index_ade20k = index_mat\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_name = os.path.join(self.root_dir, self.index_ade20k['folder'][image_id], \n",
    "                                self.index_ade20k['filename'][image_id])\n",
    "        img_info = utils.loadAde20K(img_name)\n",
    "        \n",
    "        image = io.read_image(img_info['img_name']).float()\n",
    "        class_mask = Image.fromarray(img_info['class_mask'], mode='I')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(class_mask)\n",
    "            \n",
    "        sample = (image, label)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "input_size = 224\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize(input_size),\n",
    "                transforms.CenterCrop(input_size),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "                transforms.Resize(input_size, interpolation=0),\n",
    "                transforms.CenterCrop(input_size),\n",
    "                transforms.ToTensor()\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 4\n",
    "batch_size = 2\n",
    "training_data = SegmentationDataset(train_image_ids[:num_samples], './', index_ade20k, transform=transform, target_transform=target_transform)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=False)\n",
    "testing_data = SegmentationDataset(test_image_ids[:num_samples], './', index_ade20k, transform=transform, target_transform=target_transform)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label_arr, obj_id_map):\n",
    "    \"\"\"\n",
    "    Encode labels for evaluating loss\n",
    "    label_arr (tensor): B x 1 x H x W\n",
    "    obj_id_map: dictionary mapping label class IDs to new (0-150) range IDs\n",
    "    \"\"\"\n",
    "    convert_label_ids = lambda i: obj_id_map[i-1]\n",
    "    vect_convert_label_ids = np.vectorize(convert_label_ids)\n",
    "    encoded_label = vect_convert_label_ids(label_arr.squeeze().cpu().numpy())\n",
    "    \n",
    "    return torch.tensor(encoded_label, dtype=torch.long)\n",
    "\n",
    "def get_parameter_size(model):\n",
    "    \"\"\"\n",
    "    Return model size in terms of parameters\n",
    "    Each parameter is a float32 - 4 bytes\n",
    "    \"\"\"\n",
    "    num_params = 0\n",
    "    for p in model.parameters():\n",
    "        num_params += torch.count_nonzero(p.flatten())\n",
    "        \n",
    "    total_bytes = num_params.item() * 4\n",
    "    kb = total_bytes / 1000\n",
    "    \n",
    "    return {\"# Params\": num_params.item(),\n",
    "            \"Size in KB\": kb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, test_dataloader):\n",
    "    # testing pass\n",
    "    test_start = time.time()\n",
    "    running_accuracy = 0\n",
    "    running_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images = images.to(device)\n",
    "            output = model(images)['out']\n",
    "            labels = encode_label(labels, obj_id_map).to(device)\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1, keepdim=True).squeeze()\n",
    "            num_correct = torch.sum((preds == labels).to(int)).item()\n",
    "            iou = IOU(labels.detach().cpu().numpy().reshape(-1), preds.detach().cpu().numpy().reshape(-1), average='weighted')\n",
    "            print('Testing accuracy: {}'.format(num_correct/(224*224*len(images))))\n",
    "            print('Testing IOU score: {}'.format(iou))\n",
    "            running_accuracy += num_correct/(224*224*len(images))\n",
    "            running_iou += iou\n",
    "\n",
    "    print(\"Testing time: {} seconds\".format(time.time() - test_start))\n",
    "\n",
    "    return {\"Testing pixel accuracy\": running_accuracy / len(test_dataloader),\n",
    "            \"Testing IOU accuracy\": running_iou / len(test_dataloader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdPruning(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def compute_mask(self, tensor, default_mask):\n",
    "        return torch.abs(tensor) > self.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load teacher models\n",
    "teacher_model_1 = models.segmentation.fcn_resnet50(pretrained=False, num_classes=151).to(device)\n",
    "teacher_model_1.load_state_dict(torch.load('../scene_seg_models/fcn_resnet_50/epochs_20_weights.pkl', map_location=torch.device('cpu')))\n",
    "teacher_model_2 = models.segmentation.deeplabv3_resnet50(pretrained=False, num_classes=151).to(device)\n",
    "teacher_model_2.load_state_dict(torch.load('../scene_seg_models/deeplab_resnet_50/epochs_20_weights.pkl', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(teacher_model_1) # fcn resnet 50\n",
    "\n",
    "params_to_prune = [(module, \"weight\") for _, module in model.named_modules() if isinstance(module, torch.nn.Conv2d)]\n",
    "prune.global_unstructured(params_to_prune, pruning_method=ThresholdPruning, threshold=0.025)\n",
    "for _, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.remove(module, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, teacher_models, train_dataloader, test_dataloader, obj_id_map, epochs=5, lr=0.1, momentum=0.8):\n",
    "\n",
    "    train_start = time.time()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    criterion_soft = torch.nn.CrossEntropyLoss()\n",
    "    criterion_hard = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # training pass\n",
    "        running_loss = 0\n",
    "        for images, labels in train_dataloader:\n",
    "            images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            shape, full_tensor = None, None\n",
    "            for tm in teacher_models:\n",
    "                soft_label = tm(images)['out']\n",
    "                if shape is None:\n",
    "                    shape = soft_label.size()\n",
    "                    full_tensor = torch.zeros(shape)\n",
    "                full_tensor = full_tensor.add(soft_label)\n",
    "            soft_label_outputs = torch.div(full_tensor, len(teacher_models))\n",
    "            label_probs = torch.nn.functional.softmax(soft_label_outputs, dim=1)\n",
    "            soft_labels = torch.argmax(label_probs, dim=1, keepdim=True).squeeze()\n",
    "                        \n",
    "            output = model(images)['out']\n",
    "            labels = encode_label(labels, obj_id_map).to(device)\n",
    "            loss_soft = criterion_soft(output, soft_labels)\n",
    "            loss_hard = criterion_hard(output, labels)\n",
    "            loss = sum([loss_soft, loss_hard])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            print('Batch finished...')\n",
    "        print('Training loss: {}'.format(running_loss/len(train_dataloader)))\n",
    "        print(\"Training time: {} seconds\".format(time.time() - epoch_start))\n",
    "#         torch.save(model, result_path+'/epochs_{}_model.pkl'.format(i+1))\n",
    "\n",
    "        # testing pass\n",
    "        test_start = time.time()\n",
    "        running_accuracy = 0\n",
    "        running_iou = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_dataloader:\n",
    "                images = images.to(device)\n",
    "                output = model(images)['out']\n",
    "                labels = encode_label(labels, obj_id_map).to(device)\n",
    "                probs = torch.nn.functional.softmax(output, dim=1)\n",
    "                preds = torch.argmax(probs, dim=1, keepdim=True).squeeze()\n",
    "                num_correct = torch.sum((preds == labels).to(int)).item()\n",
    "                iou = IOU(labels.detach().cpu().numpy().reshape(-1), preds.detach().cpu().numpy().reshape(-1), average='weighted')\n",
    "                print('Testing accuracy: {}'.format(num_correct/(224*224*len(images))))\n",
    "                print('Testing IOU score: {}'.format(iou))\n",
    "                running_accuracy += num_correct/(224*224*len(images))\n",
    "                running_iou += iou\n",
    "        print(\"Testing time: {} seconds\".format(time.time() - test_start))\n",
    "        print('-----> Overall testing pixel accuracy: {}'.format(running_accuracy / len(test_dataloader)))\n",
    "        print('-----> Overall testing IOU accuracy: {}'.format(running_iou / len(test_dataloader)))\n",
    "        print(\"Epoch completed in {} seconds.\".format(time.time() - epoch_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch finished...\n",
      "Batch finished...\n",
      "Training loss: 11.110483646392822\n",
      "Training time: 12.232490301132202 seconds\n",
      "Testing accuracy: 0.31869818239795916\n",
      "Testing IOU score: 0.12904685533053906\n",
      "Testing accuracy: 0.11647002551020408\n",
      "Testing IOU score: 0.042331315619023\n",
      "Testing time: 2.373562812805176 seconds\n",
      "-----> Overall testing pixel accuracy: 0.21758410395408162\n",
      "-----> Overall testing IOU accuracy: 0.08568908547478103\n",
      "Epoch completed in 14.60613489151001 seconds.\n",
      "Batch finished...\n",
      "Batch finished...\n",
      "Training loss: 11.278098106384277\n",
      "Training time: 12.46445631980896 seconds\n",
      "Testing accuracy: 0.3963149713010204\n",
      "Testing IOU score: 0.18301133116041066\n",
      "Testing accuracy: 0.33655532525510207\n",
      "Testing IOU score: 0.12749976503985794\n",
      "Testing time: 2.3401072025299072 seconds\n",
      "-----> Overall testing pixel accuracy: 0.36643514827806123\n",
      "-----> Overall testing IOU accuracy: 0.1552555481001343\n",
      "Epoch completed in 14.804692268371582 seconds.\n",
      "Batch finished...\n",
      "Batch finished...\n",
      "Training loss: 9.677506923675537\n",
      "Training time: 13.095371007919312 seconds\n",
      "Testing accuracy: 0.41403260522959184\n",
      "Testing IOU score: 0.18649248723803866\n",
      "Testing accuracy: 0.3535056600765306\n",
      "Testing IOU score: 0.13436962478275827\n",
      "Testing time: 2.3549578189849854 seconds\n",
      "-----> Overall testing pixel accuracy: 0.38376913265306123\n",
      "-----> Overall testing IOU accuracy: 0.16043105601039848\n",
      "Epoch completed in 15.450451850891113 seconds.\n",
      "Batch finished...\n",
      "Batch finished...\n",
      "Training loss: 8.56852102279663\n",
      "Training time: 13.24946403503418 seconds\n",
      "Testing accuracy: 0.33238998724489793\n",
      "Testing IOU score: 0.1456468641413316\n",
      "Testing accuracy: 0.3006218112244898\n",
      "Testing IOU score: 0.10492669716458942\n",
      "Testing time: 2.3963990211486816 seconds\n",
      "-----> Overall testing pixel accuracy: 0.31650589923469385\n",
      "-----> Overall testing IOU accuracy: 0.1252867806529605\n",
      "Epoch completed in 15.645960092544556 seconds.\n",
      "Batch finished...\n",
      "Batch finished...\n",
      "Training loss: 9.974436283111572\n",
      "Training time: 13.46974802017212 seconds\n",
      "Testing accuracy: 0.35227000956632654\n",
      "Testing IOU score: 0.1542717029238885\n",
      "Testing accuracy: 0.32632134885204084\n",
      "Testing IOU score: 0.1226371142271331\n",
      "Testing time: 2.493896961212158 seconds\n",
      "-----> Overall testing pixel accuracy: 0.3392956792091837\n",
      "-----> Overall testing IOU accuracy: 0.1384544085755108\n",
      "Epoch completed in 15.963765144348145 seconds.\n"
     ]
    }
   ],
   "source": [
    "test_model = copy.deepcopy(model)\n",
    "train_model(test_model, [teacher_model_1, teacher_model_2], train_dataloader, test_dataloader, obj_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
