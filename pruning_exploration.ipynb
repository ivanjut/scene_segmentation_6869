{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import jaccard_score as IOU\n",
    "from torchvision import models, transforms, io\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "import utils\n",
    "import os\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATASET_PATH = 'ADE20K_2021_17_01/'\n",
    "index_file = 'index_ade20k.pkl'\n",
    "with open('{}/{}'.format(DATASET_PATH, index_file), 'rb') as f:\n",
    "    index_ade20k = pkl.load(f)\n",
    "\n",
    "objects_mat = index_ade20k['objectPresence']\n",
    "\n",
    "# Find 150 most common object IDs and non-common object IDs\n",
    "total_object_counts = np.sum(objects_mat, axis=1)\n",
    "object_count_ids = np.argsort(total_object_counts)[::-1]\n",
    "most_common_obj_ids = object_count_ids[:150]\n",
    "irrelevant_obj_ids = object_count_ids[150:]\n",
    "# Find image IDs where no irrelevant objects appear\n",
    "irrelevant_obj_counts = np.sum(objects_mat[irrelevant_obj_ids], axis=0)\n",
    "good_image_ids = np.argwhere(irrelevant_obj_counts == 0).flatten()\n",
    "# Only common objects included\n",
    "common_objects_mat = objects_mat[np.ix_(most_common_obj_ids, good_image_ids)]\n",
    "\n",
    "# Maps {obj_ids: 0-149}\n",
    "obj_id_map = {sorted(most_common_obj_ids)[idx]: idx + 1 for idx in range(150)}\n",
    "obj_id_map[-1] = 0\n",
    "\n",
    "# Pick out images to train/evaluate on\n",
    "train_image_ids = []\n",
    "test_image_ids = []\n",
    "for i in good_image_ids:\n",
    "    if 'training' in index_ade20k['folder'][i]:\n",
    "        train_image_ids.append(i)\n",
    "    elif 'validation' in index_ade20k['folder'][i]:\n",
    "        test_image_ids.append(i)\n",
    "    else:\n",
    "        raise Exception('Invalid folder name.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_ids, root_dir, index_mat, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_ids (list): list of image IDs from ADE20K\n",
    "            root_dir (string): Directory with all the images.\n",
    "            index_mat (array): object array from index_ade20k.pkl\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            target_transform (callable, optional): Optional transform to be applied\n",
    "                on a sample segmentation label.\n",
    "        \"\"\"\n",
    "        self.image_ids = image_ids\n",
    "        self.root_dir = root_dir\n",
    "        self.index_ade20k = index_mat\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_name = os.path.join(self.root_dir, self.index_ade20k['folder'][image_id], \n",
    "                                self.index_ade20k['filename'][image_id])\n",
    "        img_info = utils.loadAde20K(img_name)\n",
    "        \n",
    "        image = io.read_image(img_info['img_name']).float()\n",
    "        class_mask = Image.fromarray(img_info['class_mask'], mode='I')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(class_mask)\n",
    "            \n",
    "        sample = (image, label)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "input_size = 224\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize(input_size),\n",
    "                transforms.CenterCrop(input_size),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "                transforms.Resize(input_size, interpolation=0),\n",
    "                transforms.CenterCrop(input_size),\n",
    "                transforms.ToTensor()\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 4\n",
    "batch_size = 2\n",
    "training_data = SegmentationDataset(train_image_ids[:num_samples], './', index_ade20k, transform=transform, target_transform=target_transform)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=False)\n",
    "testing_data = SegmentationDataset(test_image_ids[:num_samples], './', index_ade20k, transform=transform, target_transform=target_transform)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_size(model):\n",
    "    \"\"\"\n",
    "    Return model size in terms of parameters\n",
    "    Each parameter is a float32 - 4 bytes\n",
    "    \"\"\"\n",
    "    num_params = 0\n",
    "    for p in model.parameters():\n",
    "        num_params += torch.count_nonzero(p.flatten())\n",
    "        \n",
    "    total_bytes = num_params.item() / 4\n",
    "    kb = total_bytes / 1000\n",
    "    \n",
    "    return {\"# Params\": num_params.item(),\n",
    "            \"Size in KB\": kb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.segmentation.fcn_resnet50(pretrained=False, num_classes=151).to(device=device)\n",
    "model.load_state_dict(torch.load('../epochs_20_weights.pkl', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'# Params': 33023703, 'Size in KB': 8255.92575}\n"
     ]
    }
   ],
   "source": [
    "print(get_parameter_size(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label_arr, obj_id_map):\n",
    "    \"\"\"\n",
    "    Encode labels for evaluating loss\n",
    "    label_arr (tensor): B x 1 x H x W\n",
    "    \"\"\"\n",
    "    convert_label_ids = lambda i: obj_id_map[i-1]\n",
    "    vect_convert_label_ids = np.vectorize(convert_label_ids)\n",
    "    \n",
    "    encoded_label = vect_convert_label_ids(label_arr.squeeze().numpy())\n",
    "    \n",
    "    return torch.tensor(encoded_label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Epoch 1 ######\n",
      "Batch 1/2 finished... 3.6026101112365723 seconds\n",
      "Batch 2/2 finished... 3.639173746109009 seconds\n",
      "-----> Training loss: 1.014138251543045\n",
      "Training time: 7.943634986877441 seconds\n",
      "Testing pixel accuracy: 0.5658581792091837\n",
      "Testing IOU score: 0.2772640306122449\n",
      "Testing pixel accuracy: 0.4554866868622449\n",
      "Testing IOU score: 0.23334861288265307\n",
      "-----> Overall testing pixel accuracy: 0.5106724330357143\n",
      "-----> Overall testing IOU accuracy: 0.255306321747449\n",
      "Testing time: 2.5647149085998535 seconds\n",
      "Epoch completed in 10.508388996124268 seconds.\n",
      "###### Epoch 2 ######\n",
      "Batch 1/2 finished... 3.8629257678985596 seconds\n",
      "Batch 2/2 finished... 3.6279048919677734 seconds\n",
      "-----> Training loss: 0.8560846149921417\n",
      "Training time: 8.167415142059326 seconds\n",
      "Testing pixel accuracy: 0.5574477838010204\n",
      "Testing IOU score: 0.27103595344387754\n",
      "Testing pixel accuracy: 0.4573301977040816\n",
      "Testing IOU score: 0.23524194834183673\n",
      "-----> Overall testing pixel accuracy: 0.5073889907525511\n",
      "-----> Overall testing IOU accuracy: 0.25313895089285715\n",
      "Testing time: 2.465217113494873 seconds\n",
      "Epoch completed in 10.63282585144043 seconds.\n",
      "\n",
      "####################################################################################################\n",
      "DONE TRAINING in 21.14206290245056 seconds.\n",
      "####################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 2\n",
    "load_data_start = time.time()\n",
    "for i in range(epochs):\n",
    "    print('###### Epoch {} ######'.format(i+1))\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # training pass\n",
    "    running_loss = 0\n",
    "    batch_num = 0\n",
    "    for images, labels in train_dataloader:\n",
    "        batch_start = time.time()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)['out']\n",
    "        labels = encode_label(labels, obj_id_map).to(device)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        batch_num += 1\n",
    "        print('Batch {}/{} finished... {} seconds'.format(batch_num,len(train_dataloader), time.time() - batch_start))\n",
    "    print('-----> Training loss: {}'.format(running_loss/len(train_dataloader)))\n",
    "    print(\"Training time: {} seconds\".format(time.time() - epoch_start))\n",
    "#     torch.save(model.state_dict(), result_path+'/epochs_{}_weights.pkl'.format(i+1))\n",
    "\n",
    "    # testing pass\n",
    "    test_start = time.time()\n",
    "    running_accuracy = 0\n",
    "    running_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images = images.to(device)\n",
    "            output = model(images)['out']\n",
    "            labels = encode_label(labels, obj_id_map).to(device)\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1, keepdim=True)\n",
    "            num_correct = torch.sum((preds == labels).to(int)).item()\n",
    "            acc = num_correct/(input_size*input_size*len(images))\n",
    "            running_accuracy += acc\n",
    "            print('Testing pixel accuracy: {}'.format(num_correct/(input_size*input_size*len(images))))\n",
    "            iou = IOU(labels.detach().numpy().reshape(-1), preds.detach().numpy().reshape(-1))\n",
    "            running_iou += iou\n",
    "            print('Testing IOU score: {}'.format(iou))\n",
    "        print('-----> Overall testing pixel accuracy: {}'.format(running_accuracy / len(test_dataloader)))\n",
    "        print('-----> Overall testing IOU accuracy: {}'.format(running_iou / len(test_dataloader)))\n",
    "    print(\"Testing time: {} seconds\".format(time.time() - test_start))\n",
    "\n",
    "    print(\"Epoch completed in {} seconds.\".format(time.time() - epoch_start))\n",
    "\n",
    "print('\\n' + '#'*100)\n",
    "print(\"DONE TRAINING in {} seconds.\".format(time.time() - load_data_start))\n",
    "print('#'*100 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune low weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdPruning(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def compute_mask(self, tensor, default_mask):\n",
    "        return torch.abs(tensor) > self.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_copy = copy.deepcopy(model)\n",
    "thresh = 0.0025\n",
    "params_to_prune = [(module, \"weight\") for _, module in model_copy.named_modules() if isinstance(module, torch.nn.Conv2d)]\n",
    "\n",
    "prune.global_unstructured(params_to_prune, pruning_method=ThresholdPruning, threshold=thresh)\n",
    "\n",
    "for _, module in model_copy.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.remove(module, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'# Params': 33023703, 'Size in KB': 8255.92575}\n",
      "{'# Params': 26041274, 'Size in KB': 6510.3185}\n"
     ]
    }
   ],
   "source": [
    "print(get_parameter_size(model))\n",
    "print(get_parameter_size(model_copy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression Sizes\n",
    "Original: {'# Params': 33023703, 'Size in KB': 8255.92575}\n",
    "\n",
    "Threshold: 0.1 ---\n",
    "{'# Params': 76445, 'Size in KB': 19.11125} --- 99.8% compression\n",
    "\n",
    "Threshold: 0.025 ---\n",
    "{'# Params': 2211294, 'Size in KB': 552.8235} --- 93.3% compression\n",
    "\n",
    "Threshold: 0.01 ---\n",
    "{'# Params': 10571276, 'Size in KB': 2642.819} --- 68.0% compression\n",
    "\n",
    "Threshold: 0.0025 --- \n",
    "{'# Params': 26041274, 'Size in KB': 6510.3185} --- 21.1% compression\n",
    "\n",
    "Threshold: 0.001 ---\n",
    "{'# Params': 30165420, 'Size in KB': 7541.355} --- 8.66% compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not matching weights\n"
     ]
    }
   ],
   "source": [
    "matching_flag = True\n",
    "for p1, p2 in zip(model.parameters(), model_copy.parameters()):\n",
    "    if p1.data.ne(p2.data).sum() > 0:\n",
    "        matching_flag = False\n",
    "print(\"Copied weights\" if matching_flag else \"Not matching weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
