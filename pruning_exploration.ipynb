{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from torchvision import models, transforms, io\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "import utils\n",
    "import os\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATASET_PATH = 'ADE20K_2021_17_01/'\n",
    "index_file = 'index_ade20k.pkl'\n",
    "with open('{}/{}'.format(DATASET_PATH, index_file), 'rb') as f:\n",
    "    index_ade20k = pkl.load(f)\n",
    "\n",
    "objects_mat = index_ade20k['objectPresence']\n",
    "\n",
    "# Find 150 most common object IDs and non-common object IDs\n",
    "total_object_counts = np.sum(objects_mat, axis=1)\n",
    "object_count_ids = np.argsort(total_object_counts)[::-1]\n",
    "most_common_obj_ids = object_count_ids[:150]\n",
    "irrelevant_obj_ids = object_count_ids[150:]\n",
    "# Find image IDs where no irrelevant objects appear\n",
    "irrelevant_obj_counts = np.sum(objects_mat[irrelevant_obj_ids], axis=0)\n",
    "good_image_ids = np.argwhere(irrelevant_obj_counts == 0).flatten()\n",
    "# Only common objects included\n",
    "common_objects_mat = objects_mat[np.ix_(most_common_obj_ids, good_image_ids)]\n",
    "\n",
    "# Maps {obj_ids: 0-149}\n",
    "obj_id_map = {sorted(most_common_obj_ids)[idx]: idx + 1 for idx in range(150)}\n",
    "obj_id_map[-1] = 0\n",
    "\n",
    "# Pick out images to train/evaluate on\n",
    "train_image_ids = []\n",
    "test_image_ids = []\n",
    "for i in good_image_ids:\n",
    "    if 'training' in index_ade20k['folder'][i]:\n",
    "        train_image_ids.append(i)\n",
    "    elif 'validation' in index_ade20k['folder'][i]:\n",
    "        test_image_ids.append(i)\n",
    "    else:\n",
    "        raise Exception('Invalid folder name.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_ids, root_dir, index_mat, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_ids (list): list of image IDs from ADE20K\n",
    "            root_dir (string): Directory with all the images.\n",
    "            index_mat (array): object array from index_ade20k.pkl\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            target_transform (callable, optional): Optional transform to be applied\n",
    "                on a sample segmentation label.\n",
    "        \"\"\"\n",
    "        self.image_ids = image_ids\n",
    "        self.root_dir = root_dir\n",
    "        self.index_ade20k = index_mat\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_name = os.path.join(self.root_dir, self.index_ade20k['folder'][image_id], \n",
    "                                self.index_ade20k['filename'][image_id])\n",
    "        img_info = utils.loadAde20K(img_name)\n",
    "        \n",
    "        image = io.read_image(img_info['img_name']).float()\n",
    "        class_mask = Image.fromarray(img_info['class_mask'], mode='I')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(class_mask)\n",
    "            \n",
    "        sample = (image, label)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "input_size = 224\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize(input_size),\n",
    "                transforms.CenterCrop(input_size),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "                transforms.Resize(input_size, interpolation=0),\n",
    "                transforms.CenterCrop(input_size),\n",
    "                transforms.ToTensor()\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 4\n",
    "batch_size = 2\n",
    "training_data = SegmentationDataset(train_image_ids[:num_samples], './', index_ade20k, transform=transform, target_transform=target_transform)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=False)\n",
    "testing_data = SegmentationDataset(test_image_ids[:num_samples], './', index_ade20k, transform=transform, target_transform=target_transform)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.segmentation.fcn_resnet50(pretrained=False, num_classes=151).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_size(model):\n",
    "    \"\"\"\n",
    "    Return model size in terms of parameters\n",
    "    Each parameter is a float32 - 4 bytes\n",
    "    \"\"\"\n",
    "    num_params = 0\n",
    "    for p in model.parameters():\n",
    "        num_params += torch.count_nonzero(p.flatten())\n",
    "        \n",
    "    total_bytes = num_params.item() / 4\n",
    "    kb = total_bytes / 1000\n",
    "    \n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8255.7975\n"
     ]
    }
   ],
   "source": [
    "print(get_parameter_size(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied weights\n"
     ]
    }
   ],
   "source": [
    "model_copy = copy.deepcopy(model)\n",
    "\n",
    "matching_flag = True\n",
    "for p1, p2 in zip(model.parameters(), model_copy.parameters()):\n",
    "    if p1.data.ne(p2.data).sum() > 0:\n",
    "        matching_flag = False\n",
    "print(\"Copied weights\" if matching_flag else \"Not matching weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model_copy.named_modules():\n",
    "    # prune 20% of connections in all 2D-conv layers\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        p = prune.l1_unstructured(module, name='weight', amount=0.2)\n",
    "        p = prune.remove(p, name='weight')\n",
    "    # prune 40% of connections in all linear layers\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8255.7975\n",
      "6607.327\n"
     ]
    }
   ],
   "source": [
    "print(get_parameter_size(model))\n",
    "print(get_parameter_size(model_copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not matching weights\n"
     ]
    }
   ],
   "source": [
    "for p1, p2 in zip(model.parameters(), model_copy.parameters()):\n",
    "    if p1.data.ne(p2.data).sum() > 0:\n",
    "        matching_flag = False\n",
    "print(\"Copied weights\" if matching_flag else \"Not matching weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label_arr, obj_id_map):\n",
    "    \"\"\"\n",
    "    Encode labels for evaluating loss\n",
    "    label_arr (tensor): B x 1 x H x W\n",
    "    \"\"\"\n",
    "    convert_label_ids = lambda i: obj_id_map[i-1]\n",
    "    vect_convert_label_ids = np.vectorize(convert_label_ids)\n",
    "    \n",
    "    encoded_label = vect_convert_label_ids(label_arr.squeeze().numpy())\n",
    "    \n",
    "    return torch.tensor(encoded_label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Epoch 1 ######\n",
      "Batch 1/2 finished... 3.423030138015747 seconds\n",
      "Batch 2/2 finished... 3.5185799598693848 seconds\n",
      "-----> Training loss: 5.083685398101807\n",
      "Training time: 7.600463151931763 seconds\n",
      "Testing accuracy: 0.22341358418367346\n",
      "Testing accuracy: 0.2294025031887755\n",
      "-----> Overall testing accuracy: 0.22640804368622447\n",
      "Testing time: 2.2763829231262207 seconds\n",
      "Epoch completed in 9.876914978027344 seconds.\n",
      "###### Epoch 2 ######\n",
      "Batch 1/2 finished... 3.4821908473968506 seconds\n",
      "Batch 2/2 finished... 3.726228952407837 seconds\n",
      "-----> Training loss: 3.1742773056030273\n",
      "Training time: 7.864411115646362 seconds\n",
      "Testing accuracy: 0.5626694036989796\n",
      "Testing accuracy: 0.43714126275510207\n",
      "-----> Overall testing accuracy: 0.4999053332270408\n",
      "Testing time: 2.4393069744110107 seconds\n",
      "Epoch completed in 10.303752183914185 seconds.\n",
      "\n",
      "####################################################################################################\n",
      "DONE TRAINING in 20.181429862976074 seconds.\n",
      "####################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 2\n",
    "load_data_start = time.time()\n",
    "for i in range(epochs):\n",
    "    print('###### Epoch {} ######'.format(i+1))\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # training pass\n",
    "    running_loss = 0\n",
    "    batch_num = 0\n",
    "    for images, labels in train_dataloader:\n",
    "        batch_start = time.time()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)['out']\n",
    "        labels = encode_label(labels, obj_id_map).to(device)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        batch_num += 1\n",
    "        print('Batch {}/{} finished... {} seconds'.format(batch_num,len(train_dataloader), time.time() - batch_start))\n",
    "    print('-----> Training loss: {}'.format(running_loss/len(train_dataloader)))\n",
    "    print(\"Training time: {} seconds\".format(time.time() - epoch_start))\n",
    "#     torch.save(model.state_dict(), result_path+'/epochs_{}_weights.pkl'.format(i+1))\n",
    "\n",
    "    # testing pass\n",
    "    test_start = time.time()\n",
    "    running_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images = images.to(device)\n",
    "            output = model(images)['out']\n",
    "            labels = encode_label(labels, obj_id_map).to(device)\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1, keepdim=True)\n",
    "            num_correct = torch.sum((preds == labels).to(int)).item()\n",
    "            acc = num_correct/(input_size*input_size*len(images))\n",
    "            running_accuracy += acc\n",
    "            print('Testing accuracy: {}'.format(num_correct/(input_size*input_size*len(images))))\n",
    "        print('-----> Overall testing accuracy: {}'.format(running_accuracy / len(test_dataloader)))\n",
    "    print(\"Testing time: {} seconds\".format(time.time() - test_start))\n",
    "\n",
    "    print(\"Epoch completed in {} seconds.\".format(time.time() - epoch_start))\n",
    "\n",
    "print('\\n' + '#'*100)\n",
    "print(\"DONE TRAINING in {} seconds.\".format(time.time() - load_data_start))\n",
    "print('#'*100 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING LOOP DOC\n",
    "Output of running model for each image is (151, 224, 244) - 150 classes + 0 for background, 224 size of image. Currently using batch size 2 for testing, so each generated sample from dataloader is (batchsize, 151, 244, 244)\n",
    "\n",
    "Label is encoded to be a (244, 244) array of class labels. Classes are adapted to fit top 150 and are mapped ID wise via obj_id_map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
